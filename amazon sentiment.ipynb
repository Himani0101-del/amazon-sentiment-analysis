{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8ddad2-6af4-4f94-8b9c-3eb11c7267f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cd36bc-5e40-45fa-96fb-c8b170f6e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datset\n",
    "amazon=pd.read_csv(\"amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f408dbb-48e1-4d8c-bfe0-8af2834c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['review_content', 'review_title', 'rating', 'product_id', 'category']\n",
    "amazon_new = amazon[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87eb3e1-35c4-40b5-8fe7-8fbc4a05817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1465 entries, 0 to 1464\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   review_content  1465 non-null   object\n",
      " 1   review_title    1465 non-null   object\n",
      " 2   rating          1465 non-null   object\n",
      " 3   product_id      1465 non-null   object\n",
      " 4   category        1465 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 57.4+ KB\n"
     ]
    }
   ],
   "source": [
    "amazon_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9989d576-4fab-466d-ab31-c4ad9894d2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1465 entries, 0 to 1464\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   product_id           1465 non-null   object\n",
      " 1   product_name         1465 non-null   object\n",
      " 2   category             1465 non-null   object\n",
      " 3   discounted_price     1465 non-null   object\n",
      " 4   actual_price         1465 non-null   object\n",
      " 5   discount_percentage  1465 non-null   object\n",
      " 6   rating               1465 non-null   object\n",
      " 7   rating_count         1463 non-null   object\n",
      " 8   about_product        1465 non-null   object\n",
      " 9   user_id              1465 non-null   object\n",
      " 10  user_name            1465 non-null   object\n",
      " 11  review_id            1465 non-null   object\n",
      " 12  review_title         1465 non-null   object\n",
      " 13  review_content       1465 non-null   object\n",
      " 14  img_link             1465 non-null   object\n",
      " 15  product_link         1465 non-null   object\n",
      "dtypes: object(16)\n",
      "memory usage: 183.3+ KB\n"
     ]
    }
   ],
   "source": [
    "amazon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a5a17f-ad4c-4aeb-8864-5627b961cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fbd728-fdb2-433e-96b0-05d073b24c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()                        # convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)         # remove punctuation, numbers, emojis\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()        # fix multiple spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6330929-52e8-4b34-911f-aaa1ad774ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_46580\\316615009.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_new['clean_review'] = amazon_new['review_content'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "amazon_new['clean_review'] = amazon_new['review_content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b41541e-bfb0-4934-aa9c-78a31205886b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\himan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c48bd60-6027-428d-9625-8f9b0c988050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\himan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\himan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\himan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\himan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbff892-d89f-4d3c-bf54-600d16b55015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_46580\\1454819888.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_new['sentiment'] = amazon_new['clean_review'].apply(get_sentiment)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return \"Positive\"\n",
    "    elif polarity < -0.1:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "amazon_new['sentiment'] = amazon_new['clean_review'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6678559-93a7-4765-841e-9b186a2bb07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    1362\n",
       "Neutral       97\n",
       "Negative       6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_new['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c71654-d835-4292-baf5-d1001904b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e47c80-3094-4ed2-b576-0daae6f173e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 reviews\n",
    "sample_df = amazon_new.sample(n=100, random_state=42)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(sample_df['clean_review'])\n",
    "\n",
    "# Encode sentiment labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(sample_df['sentiment'])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_model = LogisticRegression(class_weight='balanced')\n",
    "log_model.fit(X_train, y_train)\n",
    "log_preds = log_model.predict(X_test)\n",
    "\n",
    "# Train SVM\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a8155d-15c9-42c3-9e49-3dd69ea2ac72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.00      0.00      0.00         2\n",
      "    Positive       0.93      1.00      0.97        28\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.47      0.50      0.48        30\n",
      "weighted avg       0.87      0.93      0.90        30\n",
      "\n",
      "SVM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.00      0.00      0.00         2\n",
      "    Positive       0.93      1.00      0.97        28\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.47      0.50      0.48        30\n",
      "weighted avg       0.87      0.93      0.90        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Compare performance\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, log_preds, target_names=encoder.classes_))\n",
    "\n",
    "print(\"SVM Performance:\")\n",
    "print(classification_report(y_test, svm_preds, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2263b558-f179-47b9-9f38-2574110872bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is  highly imbalanced, so the results are also showing that now manually fixing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d50f3e-a704-4618-a580-766205c44213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         2\n",
      "     Neutral       1.00      0.50      0.67         2\n",
      "    Positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.89      0.83      0.82         6\n",
      "weighted avg       0.89      0.83      0.82         6\n",
      "\n",
      "SVM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         2\n",
      "     Neutral       1.00      0.50      0.67         2\n",
      "    Positive       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.89      0.83      0.82         6\n",
      "weighted avg       0.89      0.83      0.82         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manually balance the dataset\n",
    "min_count = amazon_new['sentiment'].value_counts().min()\n",
    "sample_size = min(min_count, 50) \n",
    "\n",
    "\n",
    "pos = amazon_new[amazon_new['sentiment'] == 'Positive'].sample(n=sample_size, random_state=1)\n",
    "neg = amazon_new[amazon_new['sentiment'] == 'Negative'].sample(n=sample_size, random_state=2)\n",
    "neu = amazon_new[amazon_new['sentiment'] == 'Neutral'].sample(n=sample_size, random_state=3)\n",
    "balanced_df = pd.concat([pos, neg, neu])\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(balanced_df['clean_review'])\n",
    "\n",
    "# Encode sentiment labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(balanced_df['sentiment'])\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_model = LogisticRegression(class_weight='balanced')\n",
    "log_model.fit(X_train, y_train)\n",
    "log_preds = log_model.predict(X_test)\n",
    "\n",
    "# Train SVM\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "\n",
    "# Compare performance\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, log_preds, target_names=encoder.classes_))\n",
    "\n",
    "print(\"SVM Performance:\")\n",
    "print(classification_report(y_test, svm_preds, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d871a292-3285-49bc-8f1e-5a3ad90ce82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#still didnt help gonna try different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dad6e3b-0bfc-415d-be43-650133721069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70bfdfff-c137-45ed-a741-f4255d37a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      1.00      0.50         2\n",
      "     Neutral       0.00      0.00      0.00         2\n",
      "    Positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.11      0.33      0.17         6\n",
      "weighted avg       0.11      0.33      0.17         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset based on available samples\n",
    "min_count = amazon_new['sentiment'].value_counts().min()\n",
    "pos = amazon_new[amazon_new['sentiment'] == 'Positive'].sample(n=min_count, random_state=1)\n",
    "neg = amazon_new[amazon_new['sentiment'] == 'Negative'].sample(n=min_count, random_state=2)\n",
    "neu = amazon_new[amazon_new['sentiment'] == 'Neutral'].sample(n=min_count, random_state=3)\n",
    "balanced_df = pd.concat([pos, neg, neu])\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(balanced_df['clean_review'])\n",
    "\n",
    "# Encode sentiment labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(balanced_df['sentiment'])\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_test, rf_preds, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b90613a-31ee-4586-b41d-e518d1e02f97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:723: FutureWarning: Pass `objective` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:15:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"objective__colsample_bytree\", \"objective__enable_categorical\", \"objective__eval_metric\", \"objective__learning_rate\", \"objective__max_depth\", \"objective__missing\", \"objective__n_estimators\", \"objective__objective\", \"objective__random_state\", \"objective__scale_pos_weight\", \"objective__subsample\", \"objective__use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      1.00      0.67         2\n",
      "     Neutral       1.00      0.50      0.67         2\n",
      "    Positive       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.83      0.67      0.67         6\n",
      "weighted avg       0.83      0.67      0.67         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Balance the dataset based on available samples\n",
    "min_count = amazon_new['sentiment'].value_counts().min()\n",
    "pos = amazon_new[amazon_new['sentiment'] == 'Positive'].sample(n=min_count, random_state=1)\n",
    "neg = amazon_new[amazon_new['sentiment'] == 'Negative'].sample(n=min_count, random_state=2)\n",
    "neu = amazon_new[amazon_new['sentiment'] == 'Neutral'].sample(n=min_count, random_state=3)\n",
    "balanced_df = pd.concat([pos, neg, neu])\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(balanced_df['clean_review'])\n",
    "\n",
    "# Encode sentiment labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(balanced_df['sentiment'])\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "xgb_model = XGBClassifier(XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42\n",
    "))\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y_test, xgb_preds, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc25a1f1-020f-4d4e-84e3-292b1c129a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_46580\\4269241452.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_new['binary_sentiment'] = amazon_new['sentiment'].apply(\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:15:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Sentiment Classification (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Positive       1.00      0.13      0.23        31\n",
      "    Positive       0.94      1.00      0.97       409\n",
      "\n",
      "    accuracy                           0.94       440\n",
      "   macro avg       0.97      0.56      0.60       440\n",
      "weighted avg       0.94      0.94      0.92       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert to binary sentiment\n",
    "amazon_new['binary_sentiment'] = amazon_new['sentiment'].apply(\n",
    "    lambda x: 'Positive' if x == 'Positive' else 'Not Positive'\n",
    ")\n",
    "\n",
    "# Step 4: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(amazon_new['clean_review'])\n",
    "\n",
    "# Step 5: Encode binary labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(amazon_new['binary_sentiment'])  # 0 = Not Positive, 1 = Positive\n",
    "\n",
    "# Step 6: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 7: Train XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate performance\n",
    "print(\"Binary Sentiment Classification (XGBoost):\")\n",
    "print(classification_report(y_test, xgb_preds, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed434b1-9da0-4de7-a755-b27e6cf5c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_46580\\2123623656.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_new['binary_sentiment'] = amazon_new['sentiment'].apply(\n",
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:15:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Sentiment Classification (XGBoost, Undersampled):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Positive       0.73      0.77      0.75        31\n",
      "    Positive       0.76      0.71      0.73        31\n",
      "\n",
      "    accuracy                           0.74        62\n",
      "   macro avg       0.74      0.74      0.74        62\n",
      "weighted avg       0.74      0.74      0.74        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#still not working . so gonna try Undersampled Binary Sentiment Classification with XGBoost\n",
    " # Step 3: Convert to binary sentiment\n",
    "amazon_new['binary_sentiment'] = amazon_new['sentiment'].apply(\n",
    "    lambda x: 'Positive' if x == 'Positive' else 'Not Positive'\n",
    ")\n",
    "\n",
    "# Step 4: Undersample Positive class to match Not Positive\n",
    "not_positive = amazon_new[amazon_new['binary_sentiment'] == 'Not Positive']\n",
    "positive = amazon_new[amazon_new['binary_sentiment'] == 'Positive'].sample(n=len(not_positive), random_state=1)\n",
    "balanced_df = pd.concat([positive, not_positive])\n",
    "\n",
    "# Step 5: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(balanced_df['clean_review'])\n",
    "# Step 6: Encode binary labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(balanced_df['binary_sentiment'])  # 0 = Not Positive, 1 = Positive\n",
    "\n",
    "# Step 7: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Train XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate performance\n",
    "print(\"Binary Sentiment Classification (XGBoost, Undersampled):\")\n",
    "print(classification_report(y_test, xgb_preds, target_names=encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d2df55-ad8d-4f88-b312-1a29b2027e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples per class:\n",
      "binary_sentiment\n",
      "Positive        1362\n",
      "Not Positive     103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling size: 31\n",
      "✅ You're sampling 31 out of 1362 Positive reviews.\n",
      "✅ You're sampling 31 out of 103 Not Positive reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count total samples per class\n",
    "class_counts = amazon_new['binary_sentiment'].value_counts()\n",
    "print(\"Total samples per class:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Check how many you're sampling\n",
    "sample_size = 31 \n",
    "print(\"\\nSampling size:\", sample_size)\n",
    "\n",
    "# Compare with actual counts\n",
    "for sentiment, count in class_counts.items():\n",
    "    if sample_size >= count:\n",
    "        print(f\"⚠️ You're using ALL {sentiment} samples ({count}) — not a true sample.\")\n",
    "    else:\n",
    "        print(f\"✅ You're sampling {sample_size} out of {count} {sentiment} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b110c79e-19be-4b78-a994-025daba0841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Sentiment Classification (Naive Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Positive       0.83      0.61      0.70        31\n",
      "    Positive       0.69      0.87      0.77        31\n",
      "\n",
      "    accuracy                           0.74        62\n",
      "   macro avg       0.76      0.74      0.74        62\n",
      "weighted avg       0.76      0.74      0.74        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Step 4: Undersample Positive class to match Not Positive\n",
    "not_positive = amazon_new[amazon_new['binary_sentiment'] == 'Not Positive']\n",
    "positive = amazon_new[amazon_new['binary_sentiment'] == 'Positive'].sample(n=len(not_positive), random_state=1)\n",
    "balanced_df = pd.concat([positive, not_positive])\n",
    "\n",
    "# Step 5: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(balanced_df['clean_review'])\n",
    "\n",
    "# Step 6: Encode binary labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(balanced_df['binary_sentiment'])  # 0 = Not Positive, 1 = Positive\n",
    "\n",
    "# Step 7: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Train Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate performance\n",
    "print(\"Binary Sentiment Classification (Naive Bayes):\")\n",
    "print(classification_report(y_test, nb_preds, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6895e52e-1a0f-4f7f-8f7a-1038ddd5aa59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Sentiment Classification (Ensemble: Naive Bayes + XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Positive       0.73      0.77      0.75        31\n",
      "    Positive       0.76      0.71      0.73        31\n",
      "\n",
      "    accuracy                           0.74        62\n",
      "   macro avg       0.74      0.74      0.74        62\n",
      "weighted avg       0.74      0.74      0.74        62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:15:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Define individual models\n",
    "nb_model = MultinomialNB()\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Step 2: Create ensemble with hard voting\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', nb_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    voting='soft'  # Use 'soft' if you want to average probabilities\n",
    ")\n",
    "\n",
    "# Step 3: Train/test split (reuse your TF-IDF features and labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Train ensemble\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict and evaluate\n",
    "ensemble_preds = ensemble_model.predict(X_test)\n",
    "print(\"Binary Sentiment Classification (Ensemble: Naive Bayes + XGBoost):\")\n",
    "print(classification_report(y_test, ensemble_preds, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7928a7a4-1072-4735-85a7-9e5674bff54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:15:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Sentiment Classification (Ensemble: Naive Bayes + XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Positive       0.73      0.77      0.75        31\n",
      "    Positive       0.76      0.71      0.73        31\n",
      "\n",
      "    accuracy                           0.74        62\n",
      "   macro avg       0.74      0.74      0.74        62\n",
      "weighted avg       0.74      0.74      0.74        62\n",
      "\n",
      "                                      review_content predicted_sentiment\n",
      "0  Looks durable Charging is fine tooNo complains...            Positive\n",
      "1  I ordered this cable to connect my phone to An...            Positive\n",
      "2  Not quite durable and sturdy,https://m.media-a...            Positive\n",
      "3  Good product,long wire,Charges good,Nice,I bou...            Positive\n",
      "4  Bought this instead of original apple, does th...            Positive\n",
      "predicted_sentiment\n",
      "Positive        1047\n",
      "Not Positive     418\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_46580\\2409056044.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_new['predicted_sentiment'] = encoder.inverse_transform(full_preds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 8: Define models\n",
    "nb_model = MultinomialNB()\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Step 9: Create soft voting ensemble\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('nb', nb_model), ('xgb', xgb_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Step 10: Train ensemble\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 11: Evaluate on test set\n",
    "ensemble_preds = ensemble_model.predict(X_test)\n",
    "print(\"Binary Sentiment Classification (Ensemble: Naive Bayes + XGBoost):\")\n",
    "print(classification_report(y_test, ensemble_preds, target_names=encoder.classes_))\n",
    "\n",
    "# Step 12: Deploy on full dataset\n",
    "X_full = vectorizer.transform(amazon_new['clean_review'])\n",
    "full_preds = ensemble_model.predict(X_full)\n",
    "amazon_new['predicted_sentiment'] = encoder.inverse_transform(full_preds)\n",
    "\n",
    "# Step 13: View results\n",
    "print(amazon_new[['review_content', 'predicted_sentiment']].head())\n",
    "print(amazon_new['predicted_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9979192d-4088-4425-bd54-6c5c45a65327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained ensemble model\n",
    "joblib.dump(ensemble_model, 'ensemble_model.joblib')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(encoder, 'label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34c108-75f7-4ecd-838c-824f71568fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
